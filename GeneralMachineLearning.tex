\subsection{General Overview On Machine Learning}
\label{sub:GeneralML}
This section serves as a summary of general machine learning concepts
which are recommended to better understand the rest of the report.
The theory originates mainly from the machine learning course\cite{learningfromdata2012course} and the accompanying text book\cite{learningfromdata2012book}.

\subsubsection{Types of Machine Learning}
\label{ssub:TypesofMachineLearning}
As mentioned in the \nameref{sec:Introduction} (see Section~\ref{sec:Introduction}),
machine learning is a way of picking a fitting set of data to train on and calculate a model, 
such that similar data can be evaluated using the same model with high accuracy.
Generally speaking there are three popular types of machine learning:
supervised learning, reinforcement learning and unsupervised learning.
The type of learning is often for a project is often chosen on different sets criteria such as the type of data available,
and the knowledge about how the data is grouped.
More often than not the choice will lie on one single solution, something which will become clear in the following paragraphs.

\paragraph{Supervised Learning}
\label{par:SupervisedLearning}
To use supervised learning, the sample dataset used for learning must not only contain some valid input, but also the corresponding ``correct'' output.
The challenge therefore primarily lies on finding the right way of grouping or evaluating the input
such that the corresponding desired output can be achieved, by calculation on the model.
In many ways, this often makes Supervised Learning the most desired method, as there it is often easier to find a fitting solution which corresponds to what is observed on real-world data,
than when the desired output is completely unknown. It is also why, supervised learning has the most different techniques such as linear classification and support vector machines (see Section~\ref{ssub:ModelsforClassification}).

\paragraph{Reinforcement Learning}
\label{par:ReinforcementLearning}
Reinforcement learning is in many ways similar to supervised learning in that it requires both input data and corresponding output to be used.
The main difference is that while supervised learning requires the desired output to be correct, reinforcement only requires a grading of such output.
This technique is therefore often used, when there is no holistic overview regarding the distribution of data but where machine learning is till required.
\begin{exm}
Imagine a hospital where data is gathered on persons with an epidemical illness with multiple stages, and the
medical personnel would want to determine what factors could be causing such illness. As symptoms often can be misinterpreted on early stages,
it is unknown if the outcome is entirely correct but a grading of how ill a patient is can be used to aid the algorithm.
\end{exm}
In many ways reinforcement learning, also has to handle the grading additionally to finding a fitting model. 
This usually makes the models more complex, and as such harder to generalize properly\cite{sutton96generalizationin}\cite{boyan95generalizationin}.

\paragraph{Unsupervised Learning}
\label{par:UnsupervisedLearning}
The last popular type of learning is unsupervised learning, where only data input is given.
In many ways unsupervised learning tries to find a natural grouping or clustering of data,
such that input data that lies next to each other is grouped similarly.
This can be useful to e.g. separate colours from images or to find patterns in given input for further post-processing.
While it maybe hard to find real-world data that groups in obvious ways, and even harder making it fit some desired output;
Unsupervised learning has one advantage, that generalization is free. This is because unsupervised relies solely on input-processing and does not change
any output metrics and as such the models that can be used to not have to conform to an output specification.
Examples of unsupervised learning methods are k-means clustering\cite{learningfromdata2012book}, nearest neighbours\cite{scikitlearn2012nearestneighbours} and PCA\cite{smith2002tutorial}(see Section~\ref{sub:PCA} - \nameref{sub:PCA} for further explanation).

\paragraph{Selected Types of Learning}
\label{par:SelectedTypesofLearning}
To reduce the input space in our project, we resort to the use of PCA, which is as previously mentioned an unsupervised learning technique
and thus have no generalization cost. This allows us both to visualize how the variation/data is grouped, and allows us to
use supervised learning techniques to do the actual separation of data with much less noise and more performance than otherwise.
The choice of supervised learning is due to the fact that it offers the best trade-off between usefulness of output and generalization cost.

\subsubsection{Models for Classification}
\label{ssub:ModelsforClassification}
To be able to classify the resulting data from our experiments, we have chosen some simple supervised machine learning models to apply.
In the following sections we will thus present an overview of these models.
\paragraph{Linear Classification}
\label{par:LinearClassification}
The simplest type of classification is called linear classification.
The idea is simple: given an input dataset $\mathbf{X}$ consisting of data-vectors $\mathbf{x}$ with input parameters $x_1, \dots, x_n \in \mathbf{x}$ and the corresponding output parameter $y \in \left\{-1;+1\right\}$ (see Figure~\ref{fig:linearclass}), 
find a linear formula which separates the outputs given the input parameters.
The classes of output are usually denoted as either being negative (the output being $-1$) or positive (the output being $+1$). 
This makes it easier to run algorithms and is usually not a problem, because it is often possible to do a mapping from any given output format.\\
\begin{minipage}{\linewidth}
\centering
\makebox[\linewidth] {
  \includegraphics[width=0.5\textwidth]{LinearClassification.png}
}
\captionof{figure}{Linear classification algorithm running on two dimensional linear separable data}\label{fig:linearclass}
\end{minipage}
To separate the two classes of output, a given linear learning algorithm must assign weight coefficients to each input parameter.
To get a positive or negative classification out of the real-numbered output, the sign of the output is taken yielding two output classes.
The equation to be calculated therefore looks as follows:
$$ h(x) = sign\left(w_0 + w_1 \cdot x_1 + \dots + w_n \cdot x_n \right) $$
In order to allow separation of data with non-zero threshold value, an additional learning parameter $w_0$ is added.
If an artificial parameter $x_0=1$ is added this can be rewritten as:
$$ h(x) = sign\left(\sum_{i=0}^n{w_i \cdot x_i}\right) $$

\begin{algorithm}
  \begin{algorithmic}[1]
    \Require $\mathbf{X}$ a matrix of size $n\times m$ containing training data
    \Require $\mathbf{y}$ a vector of size $n$ which contains target output for each row vector $\mathbf{x} \in \mathbf{X}$
    \Ensure $\mathbf{w}$ a vector of size $n+1$ containing all the weights for target linear model
    \Function{CalculateWeights}{$\mathbf{X}, \mathbf{y}$}
    \Let{$\mathbf{w}$}{$\mathbf{0}$}
    \Repeat
    \ForAll{rows $\mathbf{x_j} \in \mathbf{X}$} 
    \Let{$y_j'$}{\Call{Sign}{$\sum_{i=0}^{n}{w_i\cdot x_{j,i}}$}}
    \EndFor
    \Let{$M$}{\Call{Misclassified}{$\mathbf{X}, \mathbf{y}, \mathbf{y'}$}}
    \Let{$\left(\mathbf{x_{m}}, y_{m}\right)$}{\Call{Any}{M}}
    \Comment{$\mathbf{x_{m}} \in \mathbf{X} \bigwedge y_{m} \in \mathbf{y}$} 
    \Let{$\mathbf{w}$}{$\mathbf{w} + y_{m} \cdot \mathbf{x_{m}}$}
    \Until{\Call{Misclassified}{$\mathbf{X}, \mathbf{y}, \mathbf{y'}$}Â = $\emptyset$}
    \EndFunction
  \end{algorithmic}
  \caption{The Perceptron Learning Algorithm}\label{alg:perceptron}
\end{algorithm}

The simplest linear algorithm is called the Perceptron Learning Algorithm (see Algorithm~\ref{alg:perceptron}). 
The idea is that the algorithm iteratively tries to adjust the weights by picking a misclassified point, and moving the separation line
such that the point is correctly classified. 

While this is normally done until the algorithm converges (i.e. until there are no misclassified points),
this might be impossible if the data is not completely separable.
In those situtations either a desired error rate $\epsilon$ is used or a hard limit on the number of iterations.

Linear classifications algorithms are generally good because the numbers of learning parameters are few, and thus they generalize well.

\paragraph{Logistic Regression}
\label{par:LogisticRegression}
While linear classification algorithms are simple and effective they have one disadvantage that they put a hard limit on output,
i.e. an output point is either positive or negative.
This makes it hard to identify borderline points, i.e. points which are close to the separation line, from those who are clearly in the separable region.

If these points are of interest, one choice could be to use Logistic Regression.
Logistic Regression is a learning model that enforces a softer limit on output than classical output by using the logistic function
\footnote{The logistic function usually results in a value $y \in \left(0;1\right)$, though it is ordinarily not a problem as
this can easily be translated to a scale corresponding to $\left(-1;1\right)$}
$\theta(s) = \frac{e^s}{1+e^s}$ instead of the sign function.

In this way the result would be a bounded real number which instead of specifying which class a point belongs to, specifies the probability of a point belonging to a specific class.
As we will see in Section \ref{par:MulticlassClassification}, this can be useful to use in Multiclass Classification where multiple classification algorithms ``vote'' on a given point.
\paragraph{Support Vector Machines}
\label{par:SupportVectorMachines}

Support Vector Machines are a set of advanced models that allows classification at low cost on generalization.
Strictly speaking, they exploit the fact that while all data is important for the learning algorithm there 
are only few points namely the neighbouring ones, also called the support vectors, which can influence the placement of the separation line (see Figure~\ref{fig:svm}).

\begin{minipage}{\linewidth}
\centering
\makebox[\linewidth] {
  \includegraphics[width=0.5\textwidth]{SupportVectorMachines.png}
}
\captionof{figure}{Support Vector Machines - Maximizing the margin to the neighbouring points}\label{fig:svm}
\end{minipage}

While all correct separation lines has same generalization bounds, intuitively a separation line with larger margin to its neighbouring points seems more attractable.
This is due to the fact that a larger margin implies less possibility of misconfiguration of out-of-sample points; in other words it is easier for a data point to get
misclassified because it ``jumped'' over the separation line when the line is close, then when there is a considerable margin to it.
The goal of Support Vector Machine-based models is thus to maximize the distance between the separation line and its neighbouring points,
something which can be achieved with great performance using quadratic programming\cite{learningfromdata2012course}\cite{frank2006algorithm}. 
These properties make Support Vector Machines an excellent linear classifier, and this is 
partly why we have chosen to use Support Vector Machines on our accumulated data in the experiment section.

It should be noted that Support Vector Machines are also widely known for the ability
to allow non-linear transformation capabilities at low generalization cost via kernel methods. As these capabilities are not further used in our project, we will not further
discuss those but the interested reader can refer to the "Learning From Data Course"\cite{learningfromdata2012course} for more information.

\paragraph{Multiclass Classification}
\label{par:MulticlassClassification}
In correlation with our experiment we have 4 lights which the test subjects are looking at, and as such we need to separate our data into 4 classes.
This means that in many ways classical classification is not enough if we want to have the full capabilities and therefore we have to resort to the use
of multiclass classification. While there are many approaches to multiclass classification,
two popular ways of doing multiclass classification are One versus One, and One versus Rest\cite{aly2005survey}\cite{scikitlearn2012multiclass}.
\begin{description}
  \item[One versus One] The idea of One versus One classification is to create $k$ binary classifiers for each $k$ classes,
    and then take a vote on each of the points such to see which classification the point fits best.
    While this is a novel approach it can perform competitively if the binary classifiers are used with a good learning model such as Support Vector Machines.
  \item[One versus Rest] In One versus Rest classification each pair of classes are compared on each point, and the class which compares favourably against the other classes
    wins the vote. While this creates a more complex and slower interaction, it usually performs better regarding classification than One versus One classification.
\end{description}

\subsubsection{Generalization}
\label{ssub:Generalization}
\paragraph{The Importance of Generalization}
\label{par:TheImportanceofGeneralization}
\paragraph{Bias -- Variance Trade-off}
\label{par:Bias-VarianceTrade-off}

\subsubsection{Error and Noise}
\label{ssub:ErrorandNoise}
\subsubsection{Validation and Testing}
\label{ssub:ValidationandTesting}
\paragraph{Cross Validation}
\label{par:CrossValidation}
\paragraph{Testing Out-of-Sample Performance}
\label{par:Testing Out-of-Sample Performance}


